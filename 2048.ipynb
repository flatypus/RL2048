{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2048 GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from two_oh_four_eight import Game\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.disable_interactive_logging()\n",
    "\n",
    "env = Game(max_moves=100000, max_score=100000)\n",
    "\n",
    "state_size = env.state_size\n",
    "action_size = env.action_size\n",
    "\n",
    "batch_size = 1024\n",
    "# rounds of play\n",
    "n_episodes = 10000\n",
    "output_dir = \"model_output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# (4,2)\n",
    "# 4 represents number of types of state (cart position, cart velocity, pole angle, and pole angular velocity)\n",
    "# 2 represents the number of types of actions (left and right)\n",
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.05\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.epsilon_min = 0.00\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(4, 4, 13)))\n",
    "        model.add(Conv2D(filters=128, kernel_size=(2,2), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        \n",
    "        model.add(Dense(64, activation=\"relu\", input_dim=self.state_size, kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(64, activation=\"relu\", kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        \n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, min(batch_size, len(self.memory)))\n",
    "        \n",
    "        batched_inputs = []\n",
    "        batched_targets = []\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                pred = self.model.predict(next_state)[0]\n",
    "                target = reward + self.gamma * np.amax(pred)\n",
    "            \n",
    "            target_f = self.model.predict(state)[0]\n",
    "            target_f[action] = target\n",
    "            \n",
    "            batched_inputs.append(state[0])\n",
    "            batched_targets.append(target_f)\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        batched_inputs = np.array(batched_inputs)\n",
    "        batched_targets = np.array(batched_targets)\n",
    "        \n",
    "        # Single fit call for the entire batch\n",
    "        self.model.fit(batched_inputs, batched_targets, epochs=1, verbose=False)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # if random number smaller than epsilon, return an 'exploratory' action\n",
    "            return random.randrange(self.action_size)\n",
    "        # otherwise, use the predicted move\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save(f\"{output_dir}/{name}\")\n",
    "        self.model.save_weights(f\"{output_dir}/{name}_weights.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "   \n",
    "agent = DQNAgent(state_size, action_size)\n",
    "best_score = 0\n",
    "try:\n",
    "    for index_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        invalid_moves = 0\n",
    "        invalid_move_first = False   \n",
    "        while not done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            print(f\"Action: {action}, Reward: {reward}\")\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            if not np.array_equal(state, next_state):\n",
    "                invalid_moves += 1\n",
    "            state = next_state\n",
    "        print(f\"Episode #{index_episode} ended with score: {env.score} and {invalid_moves} invalid moves\\n\")\n",
    "        agent.train(1024)\n",
    "        if env.score > best_score:\n",
    "            best_score = env.score\n",
    "            agent.save(f\"2048_{best_score}_{invalid_moves}_{env.highest_tile}.h5\")\n",
    "finally:\n",
    "    agent.save(\"2048_last.h5\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
