{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2048 GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from two_oh_four_eight import Game\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.disable_interactive_logging()\n",
    "\n",
    "MAX_LEN = 100000\n",
    "env = Game(max_moves=100000, max_score=100000)\n",
    "\n",
    "state_size = env.state_size\n",
    "action_size = env.action_size\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "N_EPISODES = 10000\n",
    "\n",
    "output_dir = \"model_output\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MAX_LEN)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.95\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.epsilon_min = 0.00\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(4, 4, 13)))\n",
    "        model.add(Conv2D(filters=128, kernel_size=(2,2), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "        states = np.array([np.squeeze(i[0]) for i in mini_sample])\n",
    "        actions = np.array([i[1] for i in mini_sample])\n",
    "        rewards = np.array([i[2] for i in mini_sample])\n",
    "        next_states = np.array([np.squeeze(i[3]) for i in mini_sample])\n",
    "        dones = np.array([i[4] for i in mini_sample])\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        ind = np.array([i for i in range(min(BATCH_SIZE, len(self.memory)))])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + self.gamma*(np.amax(self.model.predict(next_state.reshape((1, 4, 4, 13)))[0]))\n",
    "        target_full = self.model.predict(state.reshape((1, 4, 4, 13)))\n",
    "        target_full[0][action] = target\n",
    "        self.model.fit(state.reshape((1, 4, 4, 13)), target_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "            \n",
    "    def save(self, name):\n",
    "        self.model.save(f\"{output_dir}/{name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 0 0 \n",
      "2 0 0 2 \n",
      "8 4 0 0 \n",
      "2 8 4 0 \n",
      "Episode #0 ended with score: 932 and 13 invalid moves, highest tile: 128\n",
      "Episode #1 ended with score: 412 and 12 invalid moves, highest tile: 32\n",
      "Episode #2 ended with score: 260 and 5 invalid moves, highest tile: 32\n",
      "Episode #3 ended with score: 156 and 4 invalid moves, highest tile: 16\n",
      "Episode #4 ended with score: 1968 and 22 invalid moves, highest tile: 128\n",
      "Episode #5 ended with score: 588 and 19 invalid moves, highest tile: 64\n",
      "Episode #6 ended with score: 604 and 4 invalid moves, highest tile: 64\n",
      "Action: 0, Reward: -5.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "   \n",
    "agent = DQNAgent(state_size, action_size)\n",
    "best_score = 0\n",
    "results = []\n",
    "\n",
    "def train():\n",
    "    global best_score\n",
    "    global results\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    invalid_moves = 0\n",
    "    has_invalid_already = False\n",
    "    \n",
    "    while not done: \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "\n",
    "        for index, row in enumerate(results):\n",
    "            print(f\"Episode #{index} ended with score: {row[0]} and {row[1]} invalid moves, highest tile: {row[2]}\")\n",
    "\n",
    "        state = env.get_state()\n",
    "        action = agent.get_action(state)\n",
    "        reward, done, score = env.step(action)\n",
    "        state_new = env.get_state()\n",
    "\n",
    "        agent.train_short_memory(state, action, reward, state_new, done)\n",
    "\n",
    "        print(f\"Action: {action}, Reward: {reward}\")\n",
    "\n",
    "        if done:\n",
    "            agent.train_long_memory()\n",
    "\n",
    "        if np.array_equal(state, state_new):\n",
    "            invalid_moves += 1\n",
    "            if has_invalid_already:\n",
    "                continue\n",
    "            has_invalid_already = True\n",
    "        else:\n",
    "            has_invalid_already = False\n",
    "\n",
    "        agent.remember(state, action, reward, state_new, done)\n",
    "\n",
    "        \n",
    "    print(f\"Episode #{index_episode} ended with score: {env.score} and {invalid_moves} invalid moves\\n\")\n",
    "\n",
    "    if env.score > best_score:\n",
    "        best_score = env.score\n",
    "        agent.save(f\"2048_{best_score}_{invalid_moves}_{env.highest_tile}.keras\")\n",
    "    \n",
    "    results.append((env.score, invalid_moves, env.highest_tile))\n",
    "\n",
    "\n",
    "try:\n",
    "    for index_episode in range(N_EPISODES):\n",
    "        train()\n",
    "finally:\n",
    "    agent.save(\"2048_last.keras\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
