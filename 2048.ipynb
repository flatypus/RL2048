{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2048 GAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from two_oh_four_eight import Game\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.disable_interactive_logging()\n",
    "\n",
    "MAX_LEN = 100000\n",
    "env = Game(max_moves=100000, max_score=100000)\n",
    "\n",
    "state_size = env.state_size\n",
    "action_size = env.action_size\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "N_EPISODES = 10000\n",
    "\n",
    "output_dir = \"model_output\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=MAX_LEN)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 0.05\n",
    "        self.epsilon_decay = 0.9999\n",
    "        self.epsilon_min = 0.00\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(4, 4, 13)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(1024, activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "        states = np.array([np.squeeze(i[0]) for i in mini_sample])\n",
    "        actions = np.array([i[1] for i in mini_sample])\n",
    "        rewards = np.array([i[2] for i in mini_sample])\n",
    "        next_states = np.array([np.squeeze(i[3]) for i in mini_sample])\n",
    "        dones = np.array([i[4] for i in mini_sample])\n",
    "\n",
    "        targets = rewards + self.gamma*(np.amax(self.model.predict_on_batch(next_states), axis=1))*(1-dones)\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        ind = np.array([i for i in range(min(BATCH_SIZE, len(self.memory)))])\n",
    "        targets_full[[ind], [actions]] = targets\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + self.gamma*(np.amax(self.model.predict(next_state.reshape((1, 4, 4, 13)))[0]))\n",
    "        target_full = self.model.predict(state.reshape((1, 4, 4, 13)))\n",
    "        target_full[0][action] = target\n",
    "        self.model.fit(state.reshape((1, 4, 4, 13)), target_full, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "            \n",
    "    def save(self, name):\n",
    "        self.model.save(f\"{output_dir}/{name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 8 4 \n",
      "8 2 0 0 \n",
      "8 2 0 0 \n",
      "0 0 0 0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/flatypus/Documents/RL2048/2048.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index_episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPISODES):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         train()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     agent\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m2048_last.keras\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/flatypus/Documents/RL2048/2048.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m reward, done, score \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m state_new \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_state()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain_short_memory(state, action, reward, state_new, done)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode #\u001b[39m\u001b[39m{\u001b[39;00mindex_episode\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMove: \u001b[39m\u001b[39m{\u001b[39;00mswitch_action(action)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/flatypus/Documents/RL2048/2048.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m target_full \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(state\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m13\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m target_full[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(state\u001b[39m.\u001b[39;49mreshape((\u001b[39m1\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m13\u001b[39;49m)), target_full, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_min:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/flatypus/Documents/RL2048/2048.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon_decay\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1770\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1763\u001b[0m (\n\u001b[1;32m   1764\u001b[0m     data_handler\u001b[39m.\u001b[39m_initial_epoch,\n\u001b[1;32m   1765\u001b[0m     data_handler\u001b[39m.\u001b[39m_initial_step,\n\u001b[1;32m   1766\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_load_initial_counters_from_ckpt(\n\u001b[1;32m   1767\u001b[0m     steps_per_epoch_inferred, initial_epoch\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1769\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1770\u001b[0m \u001b[39mfor\u001b[39;49;00m epoch, iterator \u001b[39min\u001b[39;49;00m data_handler\u001b[39m.\u001b[39;49menumerate_epochs():\n\u001b[1;32m   1771\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_metrics()\n\u001b[1;32m   1772\u001b[0m     callbacks\u001b[39m.\u001b[39;49mon_epoch_begin(epoch)\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1341\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[0;32m-> 1341\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset)\n\u001b[1;32m   1342\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[1;32m   1343\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:496\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[1;32m    495\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 496\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    497\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    702\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[1;32m    707\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(fulltype\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m    742\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types)\n\u001b[1;32m    743\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 744\u001b[0m gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[0;32m~/Documents/RL2048/.venv/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3420\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3418\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3419\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3420\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3421\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[1;32m   3422\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3423\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "   \n",
    "agent = DQNAgent(state_size, action_size)\n",
    "best_score = 0\n",
    "results = []\n",
    "\n",
    "def switch_action(action):\n",
    "    return [\"up\", \"right\", \"down\", \"left\"][action]\n",
    "\n",
    "def train():\n",
    "    global best_score\n",
    "    global results\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_moves = 0 \n",
    "    invalid_moves = 0\n",
    "    has_invalid_already = False\n",
    "    \n",
    "    while not done: \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "\n",
    "        # for index, row in enumerate(results):\n",
    "        #     print(f\"Episode #{index} ended with score: {row[0]} and {row[1]} invalid moves, highest tile: {row[2]}\")\n",
    "\n",
    "        state = env.get_state()\n",
    "        action = agent.get_action(state)\n",
    "        reward, done, score = env.step(action)\n",
    "        state_new = env.get_state()\n",
    "\n",
    "        agent.train_short_memory(state, action, reward, state_new, done)\n",
    "\n",
    "        print(f\"Episode #{index_episode}:\") \n",
    "        print(f\"Move: {switch_action(action)}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"{invalid_moves}/{total_moves} moves\")\n",
    "        print(f\"Highest tile: {env.highest_tile}\")\n",
    "        print(f\"Reward: {reward}\")\n",
    "\n",
    "        if done:\n",
    "            for _ in range(10):\n",
    "                # 10 training iterations\n",
    "                agent.train_long_memory()\n",
    "\n",
    "        if np.array_equal(state, state_new):\n",
    "            invalid_moves += 1\n",
    "            if has_invalid_already:\n",
    "                continue\n",
    "            has_invalid_already = True\n",
    "        else:\n",
    "            has_invalid_already = False\n",
    "\n",
    "        total_moves += 1\n",
    "\n",
    "        agent.remember(state, action, reward, state_new, done)\n",
    "\n",
    "    if env.score > best_score:\n",
    "        best_score = env.score\n",
    "        agent.save(f\"2048_{best_score}_{invalid_moves}_{env.highest_tile}.keras\")\n",
    "    \n",
    "    results.append((env.score, invalid_moves, env.highest_tile))\n",
    "\n",
    "\n",
    "try:\n",
    "    for index_episode in range(N_EPISODES):\n",
    "        train()\n",
    "finally:\n",
    "    agent.save(\"2048_last.keras\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
